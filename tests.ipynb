{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ezlocalai Tests and Examples\n",
        "\n",
        "Simply choose your favorite model of choice from the models list and paste it into the `model` variable on the API calls. You can get a list of models below.\n",
        "\n",
        "Install OpenAI and requests:\n",
        "\n",
        "```bash\n",
        "pip install openai requests python-dotenv\n",
        "```\n",
        "\n",
        "**Note, you do not need an OpenAI API Key, the API Key is your `EZLOCALAI_API_KEY` for the server if you defined one in your `.env` file.**\n",
        "\n",
        "## Global definitions and helpers\n",
        "\n",
        "Confirm that your `DEFAULT_MODEL` is set to the model you want to use in your `.env` file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import openai\n",
        "import requests\n",
        "import time\n",
        "import os\n",
        "import re\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Set your system message, max tokens, temperature, and top p here, or use the defaults.\n",
        "SYSTEM_MESSAGE = \"The assistant is acting as a creative writer. All of your text responses are transcribed to audio and sent to the user. Be concise with all responses. After the request is fulfilled, end with </s>.\"\n",
        "DEFAULT_MAX_TOKENS = 256\n",
        "DEFAULT_TEMPERATURE = 0.5\n",
        "DEFAULT_TOP_P = 0.9\n",
        "\n",
        "# ------------------- DO NOT EDIT BELOW THIS LINE IN THIS CELL ------------------- #\n",
        "EZLOCALAI_SERVER = os.getenv(\"EZLOCALAI_SERVER\", \"http://localhost:8091\")\n",
        "EZLOCALAI_API_KEY = os.getenv(\"EZLOCALAI_API_KEY\", \"none\")\n",
        "DEFAULT_LLM = os.getenv(\"DEFAULT_LLM\", \"unsloth/Qwen3-VL-4B-Instruct-GGUF\")\n",
        "openai.base_url = f\"{EZLOCALAI_SERVER}/v1/\"\n",
        "openai.api_key = EZLOCALAI_API_KEY if EZLOCALAI_API_KEY else EZLOCALAI_SERVER\n",
        "HEADERS = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Authorization\": f\"{EZLOCALAI_API_KEY}\",\n",
        "    \"ngrok-skip-browser-warning\": \"true\",\n",
        "}\n",
        "\n",
        "\n",
        "def display_content(content):\n",
        "    global EZLOCALAI_SERVER\n",
        "    global HEADERS\n",
        "    outputs_url = f\"{EZLOCALAI_SERVER}/outputs/\"\n",
        "    try:\n",
        "        from IPython.display import Audio, display, Image, Video\n",
        "    except:\n",
        "        print(content)\n",
        "        return\n",
        "    if \"http://localhost:8091/outputs/\" in content:\n",
        "        if outputs_url != \"http://localhost:8091/outputs/\":\n",
        "            content = content.replace(\"http://localhost:8091/outputs/\", outputs_url)\n",
        "    if outputs_url in content:\n",
        "        urls = re.findall(f\"{re.escape(outputs_url)}[^\\\"' ]+\", content)\n",
        "        urls = urls[0].split(\"\\n\\n\")\n",
        "        for url in urls:\n",
        "            file_name = url.split(\"/\")[-1]\n",
        "            url = f\"{outputs_url}{file_name}\"\n",
        "            data = requests.get(url, headers=HEADERS).content\n",
        "            if url.endswith(\".jpg\") or url.endswith(\".png\"):\n",
        "                content = content.replace(url, \"\")\n",
        "                display(Image(url=url))\n",
        "            elif url.endswith(\".mp4\"):\n",
        "                content = content.replace(url, \"\")\n",
        "                display(Video(url=url, autoplay=True))\n",
        "            elif url.endswith(\".wav\"):\n",
        "                content = content.replace(url, \"\")\n",
        "                display(Audio(url=url, autoplay=True))\n",
        "    print(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Language Models\n",
        "\n",
        "Get a list of models to choose from if you don't already know what model you want to use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wait for server to come up instead of timing out.\n",
        "while True:\n",
        "    try:\n",
        "        models = requests.get(f\"{EZLOCALAI_SERVER}/v1/models\", headers=HEADERS)\n",
        "        if models.status_code == 200:\n",
        "            break\n",
        "    except:\n",
        "        pass\n",
        "    time.sleep(1)\n",
        "print(models.json())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Voices\n",
        "\n",
        "Any `wav` file in the `voices` directory will be available to use as a voice.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "voices = requests.get(f\"{EZLOCALAI_SERVER}/v1/audio/voices\", headers=HEADERS)\n",
        "print(voices.json())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vision Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = openai.chat.completions.create(\n",
        "    model=DEFAULT_LLM,  # Uses Qwen3-VL-4B which supports vision\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": \"Describe each stage of this image.\"},\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\n",
        "                        \"url\": f\"https://www.visualwatermark.com/images/add-text-to-photos/add-text-to-image-3.webp\"\n",
        "                    },\n",
        "                },\n",
        "            ],\n",
        "        },\n",
        "    ],\n",
        "    max_tokens=DEFAULT_MAX_TOKENS,\n",
        "    temperature=DEFAULT_TEMPERATURE,\n",
        "    top_p=DEFAULT_TOP_P,\n",
        ")\n",
        "display_content(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chat Completion\n",
        "\n",
        "[OpenAI API Reference](https://platform.openai.com/docs/api-reference/chat)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modify this prompt to generate different outputs\n",
        "prompt = \"Write a short poem about Pikachu with a picture.\"\n",
        "\n",
        "\n",
        "response = openai.chat.completions.create(\n",
        "    model=DEFAULT_LLM,\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    temperature=DEFAULT_TEMPERATURE,\n",
        "    max_tokens=DEFAULT_MAX_TOKENS,\n",
        "    top_p=DEFAULT_TOP_P,\n",
        "    stream=False,\n",
        "    extra_body={\"system_message\": SYSTEM_MESSAGE},\n",
        ")\n",
        "display_content(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Completion\n",
        "\n",
        "[OpenAI API Reference](https://platform.openai.com/docs/api-reference/completions/create)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modify this prompt to generate different outputs\n",
        "prompt = \"Write a haiku about the future.\"\n",
        "\n",
        "completion = openai.completions.create(\n",
        "    model=DEFAULT_LLM,\n",
        "    prompt=prompt,\n",
        "    temperature=DEFAULT_TEMPERATURE,\n",
        "    max_tokens=DEFAULT_MAX_TOKENS,\n",
        "    top_p=DEFAULT_TOP_P,\n",
        "    n=1,\n",
        "    stream=False,\n",
        "    extra_body={\"system_message\": SYSTEM_MESSAGE},\n",
        ")\n",
        "display_content(completion.choices[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cloning Text to Speech\n",
        "\n",
        "Any `wav` file in the `voices` directory can be used as a voice.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import base64\n",
        "import IPython.display as ipd\n",
        "\n",
        "prompt = \"Write a short poem about vikings with a picture.\"\n",
        "os.makedirs(\"temp\", exist_ok=True)\n",
        "audio_path = os.path.join(os.getcwd(), \"temp\", f\"test-speech.wav\")\n",
        "speech_file_path = Path(audio_path)\n",
        "tts_response = openai.audio.speech.create(\n",
        "    model=\"tts-1\",\n",
        "    voice=\"DukeNukem\",\n",
        "    input=prompt,\n",
        "    extra_body={\"language\": \"en\"},\n",
        ")\n",
        "audio_content = base64.b64decode(tts_response.content)\n",
        "speech_file_path.write_bytes(audio_content)\n",
        "with open(audio_path, \"wb\") as audio_file:\n",
        "    audio_file.write(audio_content)\n",
        "\n",
        "ipd.Audio(speech_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Audio to Text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(audio_path, \"rb\") as audio_file:\n",
        "    transcription = openai.audio.transcriptions.create(model=\"base\", file=audio_file)\n",
        "\n",
        "print(transcription.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Upload a Voice\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "upload_headers = HEADERS.copy()\n",
        "del upload_headers[\"Content-Type\"]\n",
        "with open(audio_path, \"rb\") as audio_file:\n",
        "    files = {\"file\": (\"test-speech.wav\", audio_file, \"audio/wav\")}\n",
        "    data = {\"voice\": \"Test\"}\n",
        "    response = requests.post(\n",
        "        f\"{EZLOCALAI_SERVER}/v1/audio/voices\",\n",
        "        files=files,\n",
        "        data=data,\n",
        "        headers=upload_headers,\n",
        "    )\n",
        "    print(response.json())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Voice Completion Example\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We will use the audio response from a couple of cells back.\n",
        "completion = openai.completions.create(\n",
        "    model=DEFAULT_LLM,\n",
        "    prompt=tts_response.content.decode(\"utf-8\"),\n",
        "    temperature=DEFAULT_TEMPERATURE,\n",
        "    max_tokens=DEFAULT_MAX_TOKENS,\n",
        "    top_p=DEFAULT_TOP_P,\n",
        "    n=1,\n",
        "    stream=False,\n",
        "    extra_body={\n",
        "        \"system_message\": SYSTEM_MESSAGE,\n",
        "        \"audio_format\": \"wav\",\n",
        "        \"voice\": \"DukeNukem\",\n",
        "    },\n",
        ")\n",
        "\n",
        "response_text = completion.choices[0].text\n",
        "display_content(response_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate an Image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"Generate an image of a cat.\"\n",
        "response = openai.images.generate(\n",
        "    prompt=prompt,\n",
        "    model=\"ByteDance/SDXL-Lightning\",\n",
        "    response_format=\"url\",\n",
        ")\n",
        "image = response.data[0].url\n",
        "display_content(image)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
